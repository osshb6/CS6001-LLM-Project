{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ee1e7-236a-4829-8f01-83de46413f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA_DIR = Path(\"./LLM_NewDataset_phi/Data\")\n",
    "\n",
    "for jf in sorted(DATA_DIR.glob(\"*.json\")):\n",
    "    print(f\"\\n[CHECK] {jf}\")\n",
    "    try:\n",
    "        with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(\"  OK â€“ valid JSON\")\n",
    "    except Exception as e:\n",
    "        print(\"  JSON ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d3839-ddcf-4942-b1e2-5aa8393b2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_JAX\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "BASE_DIR   = Path(\"./LLM_NewDataset_phi\")  \n",
    "DATA_DIR   = BASE_DIR / \"Data\"\n",
    "RESULT_DIR = BASE_DIR / \"result\"\n",
    "MODEL_DIR  = BASE_DIR / \"models/ft_tinyllama_pmp_manual\"   # <--- changed name\n",
    "\n",
    "# BASE_DIR   = Path(\"./LLM_Dataset\")\n",
    "# DATA_DIR   = BASE_DIR / \"Data\"\n",
    "# RESULT_DIR = BASE_DIR / \"result\"\n",
    "# MODEL_DIR  = BASE_DIR / \"models/ft_qwen2.5_1.5B_pmp_manual\"\n",
    "\n",
    "\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   \n",
    "# MODEL_ID = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "MAX_NEW_TOKENS = 64\n",
    "N_SHOW          = 10\n",
    "MAX_LENGTH      = 512\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO   = 0.1\n",
    "TEST_RATIO  = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 2\n",
    "LR         = 5e-5\n",
    "\n",
    "overall_csv_path  = RESULT_DIR / \"pmp_ft_tinyllama_global_TEST_results.csv\"\n",
    "overall_xlsx_path = RESULT_DIR / \"pmp_ft_tinyllama_global_TEST_results.xlsx\"\n",
    "# overall_csv_path  = RESULT_DIR / \"pmp_ft_qwen_global_TEST_results.csv\"\n",
    "# overall_xlsx_path = RESULT_DIR / \"pmp_ft_qwen_global_TEST_results.xlsx\"\n",
    "\n",
    "\n",
    "json_files = sorted(DATA_DIR.glob(\"*.json\"))\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No JSON files found in {DATA_DIR}. \"\n",
    "        \"Put your question_*.json or Question_*_trick_160.json files there.\"\n",
    "    )\n",
    "\n",
    "all_questions = []\n",
    "for jf in json_files:\n",
    "    src_name = jf.name\n",
    "    print(f\"[INFO] Loading {jf}\")\n",
    "    with open(jf, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(f\"File {jf} must contain a list of question objects (JSON array).\")\n",
    "        for q in data:\n",
    "            q[\"source_file\"] = src_name\n",
    "            all_questions.append(q)\n",
    "\n",
    "print(f\"[INFO] Total questions loaded from {len(json_files)} files: {len(all_questions)}\")\n",
    "\n",
    "required_fields = {\"question_id\", \"question\", \"type\", \"options\", \"answer\", \"template_type\"}\n",
    "missing_fields = set()\n",
    "for q in all_questions[: min(10, len(all_questions))]:\n",
    "    for rf in required_fields:\n",
    "        if rf not in q:\n",
    "            missing_fields.add(rf)\n",
    "if missing_fields:\n",
    "    print(f\"[WARN] Some required fields are missing in sample questions: {missing_fields}\")\n",
    "else:\n",
    "    print(\"[INFO] Sample questions look structurally OK.\")\n",
    "\n",
    "def determine_format(q_obj: Dict[str, Any]) -> str:\n",
    "    t = q_obj.get(\"type\", \"\").strip().upper()\n",
    "    if \"T\" in t:   # e.g. 'T/F'\n",
    "        return \"TF\"\n",
    "    return \"MCQ\"\n",
    "\n",
    "def build_prompt_tf_for_train(q_obj, answer_letter: str) -> str:\n",
    "    return (\n",
    "        \"You are a historian. Answer the following True/False question using ONLY \"\n",
    "        \"the letter 'a' for True or 'b' for False.\\n\\n\"\n",
    "        f\"Question: {q_obj['question']}\\n\"\n",
    "        \"Options:\\n\"\n",
    "        \"A. True\\n\"\n",
    "        \"B. False\\n\\n\"\n",
    "        f\"Answer: {answer_letter}\\n\"\n",
    "    )\n",
    "\n",
    "def build_prompt_mcq_for_train(q_obj, answer_letter: str) -> str:\n",
    "    opts = \"\\n\".join(f\"{k.upper()}. {v}\" for k, v in q_obj[\"options\"].items())\n",
    "    return (\n",
    "        \"You are a historian. Choose the correct option and answer using ONLY the \"\n",
    "        \"letter (a, b, c, or d).\\n\\n\"\n",
    "        f\"Question: {q_obj['question']}\\n{opts}\\n\\n\"\n",
    "        f\"Answer: {answer_letter}\\n\"\n",
    "    )\n",
    "\n",
    "def build_prompt_tf_for_infer(q_obj) -> str:\n",
    "    return (\n",
    "        \"You are a historian. Answer the following True/False question using ONLY \"\n",
    "        \"the letter 'a' for True or 'b' for False.\\n\\n\"\n",
    "        f\"Question: {q_obj['question']}\\n\"\n",
    "        \"Options:\\n\"\n",
    "        \"A. True\\n\"\n",
    "        \"B. False\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def build_prompt_mcq_for_infer(q_obj) -> str:\n",
    "    opts = \"\\n\".join(f\"{k.upper()}. {v}\" for k, v in q_obj[\"options\"].items())\n",
    "    return (\n",
    "        \"You are a historian. Choose the correct option and answer using ONLY the \"\n",
    "        \"letter (a, b, c, or d).\\n\\n\"\n",
    "        f\"Question: {q_obj['question']}\\n{opts}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def extract_letter_from_output(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    if t and t[0].lower() in [\"a\", \"b\", \"c\", \"d\"]:\n",
    "        return t[0].lower()\n",
    "    m = re.search(r\"\\b([A-D])\\b\", t, re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).lower()\n",
    "    return \"unknown\"\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "by_template: Dict[str, list] = {}\n",
    "for q in all_questions:\n",
    "    ttype = q.get(\"template_type\", \"unknown\")\n",
    "    by_template.setdefault(ttype, []).append(q)\n",
    "\n",
    "train_data = []\n",
    "val_data   = []\n",
    "test_data  = []\n",
    "\n",
    "for ttype, qlist in by_template.items():\n",
    "    random.shuffle(qlist)\n",
    "    n = len(qlist)\n",
    "\n",
    "    n_train = int(TRAIN_RATIO * n)\n",
    "    n_val   = int(VAL_RATIO * n)\n",
    "    n_test  = n - n_train - n_val\n",
    "\n",
    "    t_train = qlist[:n_train]\n",
    "    t_val   = qlist[n_train:n_train + n_val]\n",
    "    t_test  = qlist[n_train + n_val:]\n",
    "\n",
    "    train_data.extend(t_train)\n",
    "    val_data.extend(t_val)\n",
    "    test_data.extend(t_test)\n",
    "\n",
    "    print(\n",
    "        f\"[SPLIT] template_type={ttype}: \"\n",
    "        f\"train={len(t_train)}, val={len(t_val)}, test={len(t_test)} (total={n})\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"[SPLIT] TOTAL train={len(train_data)}, \"\n",
    "    f\"val={len(val_data)}, test={len(test_data)}, all={len(all_questions)}\"\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Loading base model: {MODEL_ID}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "# (Optional, but recommended to avoid warnings during training)\n",
    "if hasattr(base_model.config, \"use_cache\"):\n",
    "    base_model.config.use_cache = False\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "class PMPDataset(Dataset):\n",
    "    def __init__(self, questions, tokenizer, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_obj = self.questions[idx]\n",
    "        fmt = determine_format(q_obj)\n",
    "        gold_letter = q_obj[\"answer\"].lower()\n",
    "\n",
    "        if fmt == \"TF\":\n",
    "            text = build_prompt_tf_for_train(q_obj, gold_letter)\n",
    "        else:\n",
    "            text = build_prompt_mcq_for_train(q_obj, gold_letter)\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "train_dataset = PMPDataset(train_data, tokenizer, max_length=MAX_LENGTH)\n",
    "val_dataset   = PMPDataset(val_data,   tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "total_steps = NUM_EPOCHS * max(len(train_loader), 1)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps) if total_steps > 0 else 0,\n",
    "    num_training_steps=total_steps if total_steps > 0 else 1,\n",
    ")\n",
    "\n",
    "print(\"[INFO] Starting manual LoRA fine-tuning...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / max(len(train_loader), 1)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / max(len(val_loader), 1)\n",
    "\n",
    "    print(\n",
    "        f\"[EPOCH {epoch+1}/{NUM_EPOCHS}] \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"[INFO] Training completed.\")\n",
    "\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "print(f\"[INFO] Saved fine-tuned LoRA model to {MODEL_DIR}\")\n",
    "\n",
    "print(\"[INFO] Reloading base model + LoRA adapter for TEST evaluation...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "if hasattr(base_model.config, \"use_cache\"):\n",
    "    base_model.config.use_cache = False\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model = PeftModel.from_pretrained(model, MODEL_DIR).to(device)\n",
    "model.eval()\n",
    "print(\"[INFO] Fine-tuned model ready for TEST set.\")\n",
    "\n",
    "def ask_model(prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "    full = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return full[len(prompt):].strip()\n",
    "\n",
    "overall_correct = 0\n",
    "overall_total = 0\n",
    "per_template_stats: Dict[str, Dict[str, int]] = {}\n",
    "per_format_stats: Dict[str, Dict[str, int]] = {}\n",
    "per_file_stats: Dict[str, Dict[str, int]] = {}\n",
    "results_rows = []\n",
    "examples_shown = 0\n",
    "\n",
    "print(f\"[INFO] Evaluating on TEST set of size {len(test_data)}\")\n",
    "\n",
    "for q in test_data:\n",
    "    qid = q.get(\"question_id\")\n",
    "    q_type = q.get(\"template_type\", \"unknown\")\n",
    "    fmt = determine_format(q)\n",
    "    question_text = q[\"question\"]\n",
    "    gold_letter = q[\"answer\"].lower()\n",
    "    src_file = q.get(\"source_file\", \"unknown\")\n",
    "\n",
    "    if fmt == \"TF\":\n",
    "        prompt = build_prompt_tf_for_infer(q)\n",
    "    else:\n",
    "        prompt = build_prompt_mcq_for_infer(q)\n",
    "\n",
    "    raw_answer = ask_model(prompt)\n",
    "    pred_letter = extract_letter_from_output(raw_answer)\n",
    "    is_correct = int(pred_letter == gold_letter)\n",
    "\n",
    "    overall_correct += is_correct\n",
    "    overall_total += 1\n",
    "\n",
    "    per_template_stats.setdefault(q_type, {\"correct\": 0, \"total\": 0})\n",
    "    per_template_stats[q_type][\"correct\"] += is_correct\n",
    "    per_template_stats[q_type][\"total\"] += 1\n",
    "\n",
    "    per_format_stats.setdefault(fmt, {\"correct\": 0, \"total\": 0})\n",
    "    per_format_stats[fmt][\"correct\"] += is_correct\n",
    "    per_format_stats[fmt][\"total\"] += 1\n",
    "\n",
    "    per_file_stats.setdefault(src_file, {\"correct\": 0, \"total\": 0})\n",
    "    per_file_stats[src_file][\"correct\"] += is_correct\n",
    "    per_file_stats[src_file][\"total\"] += 1\n",
    "\n",
    "    results_rows.append({\n",
    "        \"question_id\": qid,\n",
    "        \"source_file\": src_file,\n",
    "        \"template_type\": q_type,\n",
    "        \"question_format\": fmt,\n",
    "        \"question\": question_text,\n",
    "        \"gold_answer_letter\": gold_letter,\n",
    "        \"gold_answer_text\": q[\"options\"][gold_letter],\n",
    "        \"model_raw_answer\": raw_answer,\n",
    "        \"model_parsed_letter\": pred_letter,\n",
    "        \"model_parsed_text\": q[\"options\"].get(pred_letter, \"\") if pred_letter in q[\"options\"] else \"\",\n",
    "        \"is_correct\": is_correct,\n",
    "    })\n",
    "\n",
    "    if examples_shown < N_SHOW:\n",
    "        print(\"\\n==============================\")\n",
    "        print(f\"[TEST EXAMPLE] QID {qid} | File: {src_file} | Type: {q_type} | Format: {fmt}\")\n",
    "        print(\"Q:\", question_text)\n",
    "        for k, v in q[\"options\"].items():\n",
    "            print(f\"  {k.upper()}. {v}\")\n",
    "        print(\"MODEL RAW:\", raw_answer)\n",
    "        print(\"PRED LETTER:\", pred_letter, \"| GOLD LETTER:\", gold_letter, \"| CORRECT?\", bool(is_correct))\n",
    "        examples_shown += 1\n",
    "\n",
    "# ========= 13. Summary =========\n",
    "overall_acc = overall_correct / max(overall_total, 1)\n",
    "print(\"\\n==============================\")\n",
    "print(f\"[RESULT] GLOBAL TEST accuracy: {overall_acc:.3f}\")\n",
    "print(f\"[RESULT] Total TEST questions evaluated: {overall_total}\")\n",
    "\n",
    "print(\"\\n[RESULT] Accuracy per template_type (TEST):\")\n",
    "for t, stats in per_template_stats.items():\n",
    "    acc = stats[\"correct\"] / max(stats[\"total\"], 1)\n",
    "    total_q = stats[\"total\"]\n",
    "    print(f\"  - {t}: {acc:.3f}  (n={total_q})\")\n",
    "\n",
    "print(\"\\n[RESULT] Accuracy per question_format (TEST):\")\n",
    "for ffmt, stats in per_format_stats.items():\n",
    "    acc = stats[\"correct\"] / max(stats[\"total\"], 1)\n",
    "    total_q = stats[\"total\"]\n",
    "    print(f\"  - {ffmt}: {acc:.3f}  (n={total_q})\")\n",
    "\n",
    "print(\"\\n[RESULT] Accuracy per source_file (TEST):\")\n",
    "for fname, stats in per_file_stats.items():\n",
    "    acc = stats[\"correct\"] / max(stats[\"total\"], 1)\n",
    "    total_q = stats[\"total\"]\n",
    "    print(f\"  - {fname}: {acc:.3f}  (n={total_q})\")\n",
    "\n",
    "# ========= 14. Save GLOBAL TEST results =========\n",
    "results_df = pd.DataFrame(results_rows)\n",
    "results_df.to_csv(overall_csv_path, index=False)\n",
    "results_df.to_excel(overall_xlsx_path, index=False)\n",
    "\n",
    "print(f\"\\n[INFO] Saved GLOBAL TEST per-question results to CSV:   {overall_csv_path}\")\n",
    "print(f\"[INFO] Saved GLOBAL TEST per-question results to Excel: {overall_xlsx_path}\")\n",
    "\n",
    "# ========= 15. Save PER-FILE TEST results =========\n",
    "for fname, group_df in results_df.groupby(\"source_file\"):\n",
    "    safe_name = fname.replace(\".json\", \"\")\n",
    "    file_csv  = RESULT_DIR / f\"TEST_results_{safe_name}.csv\"\n",
    "    file_xlsx = RESULT_DIR / f\"TEST_results_{safe_name}.xlsx\"\n",
    "    group_df.to_csv(file_csv, index=False)\n",
    "    group_df.to_excel(file_xlsx, index=False)\n",
    "    print(f\"[INFO] Saved TEST results for {fname} -> {file_csv}, {file_xlsx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268343b8-42f9-423b-ad37-19f205d46885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
